{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187664a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T14:12:42.897278Z",
     "iopub.status.busy": "2025-01-25T14:12:42.896887Z",
     "iopub.status.idle": "2025-01-25T14:13:00.765547Z",
     "shell.execute_reply": "2025-01-25T14:13:00.764215Z"
    },
    "papermill": {
     "duration": 17.874072,
     "end_time": "2025-01-25T14:13:00.767391",
     "exception": false,
     "start_time": "2025-01-25T14:12:42.893319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images or labels were loaded. Please check the dataset directory and file extensions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import NASNetLarge\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from PIL import Image\n",
    "\n",
    "def log_and_print(message):\n",
    "    print(message)\n",
    "\n",
    "def load_dataset(input_dir, target_size=(331, 331)):\n",
    "    \"\"\"\n",
    "    Load and preprocess images and labels from the dataset directory.\n",
    "    Args:\n",
    "        input_dir (str): Path to the dataset directory.\n",
    "        target_size (tuple): Target size for resizing images.\n",
    "    Returns:\n",
    "        np.array, np.array: Arrays of images and corresponding labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_dirs = [os.path.join(input_dir, d) for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]\n",
    "    \n",
    "    if not class_dirs:\n",
    "        raise ValueError(f\"No subdirectories found in {input_dir}. Please check the dataset structure.\")\n",
    "    \n",
    "    for class_dir in class_dirs:\n",
    "        class_name = os.path.basename(class_dir)\n",
    "        image_paths = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).resize(target_size)\n",
    "                images.append(np.array(img))\n",
    "                labels.append(class_name)\n",
    "            except Exception as e:\n",
    "                log_and_print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    if not images or not labels:\n",
    "        raise ValueError(\"No images or labels were loaded. Please check the dataset directory and file extensions.\")\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    log_and_print(f\"Loaded {len(images)} images across {len(set(labels))} classes.\")\n",
    "    return images, labels\n",
    "\n",
    "# 2. Extract deep features\n",
    "def extract_features(images, model):\n",
    "    preprocessed_images = preprocess_input(images)\n",
    "    features = model.predict(preprocessed_images)\n",
    "    return features\n",
    "\n",
    "# 3. Apply SMOTE to balance features\n",
    "def apply_smote(features, labels):\n",
    "    smote = SMOTE()\n",
    "    features = features.reshape(features.shape[0], -1)  # Flatten features for SMOTE\n",
    "    labels_encoded = LabelEncoder().fit_transform(labels)\n",
    "    features_smote, labels_smote = smote.fit_resample(features, labels_encoded)\n",
    "    log_and_print(f\"Applied SMOTE: Original size = {len(features)}, Augmented size = {len(features_smote)}\")\n",
    "    return features_smote, labels_smote\n",
    "\n",
    "# 4. Build and train classifier\n",
    "def build_classifier(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    INPUT_DIR = \"/kaggle/input/cropped-dataset-clean/cropped_dataset (())\"\n",
    "    TARGET_SIZE = (331, 331)\n",
    "    OUTPUT_DIR = \"/kaggle/working/final_model\"\n",
    "\n",
    "    # Step 1: Load dataset\n",
    "    try:\n",
    "        images, labels = load_dataset(INPUT_DIR, target_size=TARGET_SIZE)\n",
    "    except ValueError as e:\n",
    "        log_and_print(str(e))\n",
    "        return\n",
    "\n",
    "    # Shuffle and split the dataset\n",
    "    images, labels = shuffle(images, labels, random_state=42)\n",
    "\n",
    "    if len(images) < 10:\n",
    "        log_and_print(\"Dataset is too small to split. Please ensure the dataset has enough samples.\")\n",
    "        return\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        images, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        log_and_print(\"Train or test set is empty. Adjust `test_size` or add more data.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Extract deep features using NASNetLarge\n",
    "    base_model = NASNetLarge(weights=\"imagenet\", include_top=False, pooling=\"avg\", input_shape=(331, 331, 3))\n",
    "    feature_model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "    log_and_print(\"Extracting deep features...\")\n",
    "    train_features = extract_features(X_train, feature_model)\n",
    "    test_features = extract_features(X_test, feature_model)\n",
    "\n",
    "    # Step 3: Apply SMOTE to balance training features\n",
    "    smote_features, smote_labels = apply_smote(train_features, y_train)\n",
    "\n",
    "    # Step 4: Convert labels to one-hot encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    smote_labels_encoded = label_encoder.fit_transform(smote_labels)\n",
    "    smote_labels_one_hot = to_categorical(smote_labels_encoded)\n",
    "\n",
    "    test_labels_encoded = label_encoder.transform(y_test)\n",
    "    test_labels_one_hot = to_categorical(test_labels_encoded)\n",
    "\n",
    "    # Step 5: Build and train classifier\n",
    "    log_and_print(\"Building and training classifier...\")\n",
    "    classifier = build_classifier(input_shape=smote_features.shape[1:], num_classes=len(label_encoder.classes_))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    classifier.fit(\n",
    "        smote_features,\n",
    "        smote_labels_one_hot,\n",
    "        validation_data=(test_features, test_labels_one_hot),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Step 6: Save model\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    model_path = os.path.join(OUTPUT_DIR, \"facial_expression_model.h5\")\n",
    "    classifier.save(model_path)\n",
    "    log_and_print(f\"Model saved at {model_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8260dd",
   "metadata": {
    "papermill": {
     "duration": 0.001512,
     "end_time": "2025-01-25T14:13:00.771132",
     "exception": false,
     "start_time": "2025-01-25T14:13:00.769620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6546039,
     "sourceId": 10577842,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.793597,
   "end_time": "2025-01-25T14:13:02.396835",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-25T14:12:39.603238",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
